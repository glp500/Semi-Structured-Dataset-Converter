{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce05d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "def sql_to_dataframes(sql_file_path):\n",
    "    with open(sql_file_path, 'r', encoding='utf-8') as f:\n",
    "        sql = f.read()\n",
    "\n",
    "    tables = {}\n",
    "\n",
    "    # Step 1: Parse CREATE TABLE statements to get column order\n",
    "    create_table_matches = re.findall(\n",
    "        r'CREATE TABLE [`\\[]?(\\w+)[`\\]]?\\s*\\((.*?)\\);',\n",
    "        sql, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for table_name, columns_block in create_table_matches:\n",
    "        columns = []\n",
    "        lines = [line.strip().rstrip(',') for line in columns_block.strip().splitlines()]\n",
    "        for line in lines:\n",
    "            if line.upper().startswith(('PRIMARY KEY', 'UNIQUE', 'KEY', 'CONSTRAINT', 'FOREIGN')):\n",
    "                continue\n",
    "            match = re.match(r'[`[]?(\\w+)[`\\]]?\\s', line)\n",
    "            if match:\n",
    "                columns.append(match.group(1))\n",
    "        tables[table_name] = {'columns': columns, 'rows': []}\n",
    "\n",
    "    # Step 2: Parse INSERT INTO with single or multiple VALUES\n",
    "    insert_statements = re.findall(\n",
    "        r'INSERT INTO [`\\[]?(\\w+)[`\\]]?\\s*\\((.*?)\\)\\s*VALUES\\s*(.*?);',\n",
    "        sql, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for table_name, col_block, values_block in insert_statements:\n",
    "        if table_name not in tables:\n",
    "            print(f\"‚ö†Ô∏è Skipping unknown table {table_name}\")\n",
    "            continue\n",
    "\n",
    "        insert_columns = [c.strip(' `[]') for c in col_block.split(',')]\n",
    "\n",
    "        # Extract individual value tuples\n",
    "        value_tuples = re.findall(r'\\((.*?)\\)', values_block, re.DOTALL)\n",
    "        for value_group in value_tuples:\n",
    "            cleaned = value_group.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "            reader = csv.reader([cleaned], skipinitialspace=True)\n",
    "            try:\n",
    "                values = next(reader)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error parsing row in {table_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Map to full column order\n",
    "            full_row = [''] * len(tables[table_name]['columns'])\n",
    "            for col, val in zip(insert_columns, values):\n",
    "                if col in tables[table_name]['columns']:\n",
    "                    idx = tables[table_name]['columns'].index(col)\n",
    "                    full_row[idx] = val\n",
    "            tables[table_name]['rows'].append(full_row)\n",
    "\n",
    "    # Step 3: Convert to pandas DataFrames\n",
    "    dataframes = {}\n",
    "    for table_name, data in tables.items():\n",
    "        df = pd.DataFrame(data['rows'], columns=data['columns'])\n",
    "        dataframes[table_name] = df\n",
    "        print(f\"‚úÖ Loaded {table_name}: {df.shape[0]} rows\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "def save_dataframes_to_csv(dataframes, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for table_name, df in dataframes.items():\n",
    "        output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Saved {table_name}.csv with {len(df)} rows\")\n",
    "\n",
    "    print(f\"\\nüéâ All CSVs saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6925bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 01_clerus_bio: 12628 rows\n",
      "‚úÖ Loaded 02_drc_org: 14489 rows\n",
      "‚úÖ Loaded 03_clerus_exam: 0 rows\n",
      "‚úÖ Loaded 11_clerus_alt_name: 12595 rows\n",
      "‚úÖ Loaded 12_clerus_role: 34165 rows\n",
      "‚úÖ Loaded 21_birth_alt_place: 0 rows\n",
      "‚úÖ Loaded 22_baptism_alt_place: 0 rows\n",
      "‚úÖ Loaded 23_death_alt_place: 0 rows\n",
      "‚úÖ Loaded 24_burried_alt_place: 0 rows\n",
      "‚úÖ Loaded 25_role_alt_place: 0 rows\n",
      "‚úÖ Loaded 26_residence_alt_place: 0 rows\n",
      "‚úÖ Saved 01_clerus_bio.csv with 12628 rows\n",
      "‚úÖ Saved 02_drc_org.csv with 14489 rows\n",
      "‚úÖ Saved 03_clerus_exam.csv with 0 rows\n",
      "‚úÖ Saved 11_clerus_alt_name.csv with 12595 rows\n",
      "‚úÖ Saved 12_clerus_role.csv with 34165 rows\n",
      "‚úÖ Saved 21_birth_alt_place.csv with 0 rows\n",
      "‚úÖ Saved 22_baptism_alt_place.csv with 0 rows\n",
      "‚úÖ Saved 23_death_alt_place.csv with 0 rows\n",
      "‚úÖ Saved 24_burried_alt_place.csv with 0 rows\n",
      "‚úÖ Saved 25_role_alt_place.csv with 0 rows\n",
      "‚úÖ Saved 26_residence_alt_place.csv with 0 rows\n",
      "\n",
      "üéâ All CSVs saved to: output/csv_tables\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load SQL dump into DataFrames\n",
    "dataframes = sql_to_dataframes(\"input/CLERUS_v1.sql\")\n",
    "\n",
    "# Step 2: Save each DataFrame to CSV\n",
    "save_dataframes_to_csv(dataframes, \"output/csv_tables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
